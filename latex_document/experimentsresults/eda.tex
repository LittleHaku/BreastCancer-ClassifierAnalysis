After loading our dataset \cite{william_wolberg_breast_1993} to our \textit{Jupyter Notebook} and importing it from the \textit{Scikit-learn} library \cite{scikit_learn_sklearndatasetsload_breast_cancer_nodate}, we convert it into a \textit{Pandas}\cite{team_pandas-devpandas_2020} DataFrame because it allow us to manipulate the data more easily.

Before proceeding to look at the data, we need to perform a testing and training split, this is to avoid data leakage, 
\ab{this was already explained before, check the pdf, although the citation was not included. Please, remove this sentence from here and include the cite in one of the first definitions of leakage in the other chapters}
\ab{in general, you have to be careful and avoid repetitions. I understand you don't read the text everytime you continue writing, but people will read in a single session more or less, so these things will be obvious. I will try to pay attention to these things, but since I am also reading other reports, I may miss some things...}
which is when the model learns from the testing data causing it to overfit instead of generalize \cite{wang_machine_2020}. 
To perform the split we will use the \textit{train\_test\_split} method from the \textit{Scikit-learn} library \cite{pedregosa_scikit-learn_2011}. \ab{these technical details fit better in the implementation chapter. If this was not explained there, I would ignore it and simply say that we split the data according to these percentages}
The percentage of the data that will be used for testing will be 30\% and 70\% for training, this is because 80-20 and 70-30 are the most common splits used \cite{racz_effect_2021} 
and since in this case we opted for the latter in order to have more data to be able \ab{this previous sentence is too complex and confusing, please rephrase it}
to test the classifiers and know if they generalize well.

Now that we have our data loaded and split in a DataFrame, we can begin the \ac{eda}. We will divide the process into two parts, the first part will be the descriptive statistics and the second part will be the data visualization.

\subsection{Descriptive Statistics}

The first step in the \ac{eda} is to look at the numerical description of the data, this will allow us to have a general idea of the data we are working with. To do this we will use the \textit{describe} method from the \textit{Pandas} library. From this quick statistical summary, we obtain the table available in Table \ref{TAB:FullDescriptiveStats}, which shows the count, mean, standard deviation, minimum, 25\%, 50\%, 75\%, and maximum values of each feature in the dataset, since the dataset has 30 features, we will only show the features included in the Table \ref{TAB:ShortStats}; we encourage the reader to look at the full table in the Table \ref{TAB:FullDescriptiveStats}.

\begin{table}[short]{TAB:ShortStats}{Descriptive statistics of only some features of the dataset to fit the table on the page.}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{measure} & \textbf{worst area} & \textbf{mean area} & \textbf{\bfseries mean fractal dimension} \\
        \hline\hline
        \textbf{count}   & 398.0               & 398.0              & 398.0                                     \\
        \hline
        \textbf{mean}    & 876.2               & 655.9              & 0.1                                       \\
        \hline
        \textbf{std}     & 525.5               & 333.5              & 0.1                                       \\
        \hline
        \textbf{min}     & 185.2               & 143.5              & 0.0                                       \\
        \hline
        \textbf{25\%}    & 524.1               & 432.1              & 0.1                                       \\
        \hline
        \textbf{50\%}    & 688.1               & 557.4              & 0.1                                       \\
        \hline
        \textbf{75\%}    & 1063.5              & 782.7              & 0.1                                       \\
        \hline
        \textbf{max}     & 3216.0              & 2250.0             & 0.1                                       \\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item The median (50\%) is usually smaller 
    % removed 'a bit' because it is too informal
    than the mean, which would mean that the data is lightly skewed, we will dig into this result in the next section.
    \item The difference between the 75\% and the maximum value is more than double the number in some cases (worst area 75\% = 1063 and worst area max = 3216), which would mean that there are some outliers in the data, this will also be seen later.
    \item The range in some values is pretty large, for example in the mean area we go from a min of 143.5 to a max of 2250.0, this means that we will have to scale the data in order to be able to use it with some classifiers.
    \item The standard deviation is also quite large in some cases, for example, in the worst area, we have a standard deviation of 525.5, this points out that the data is spread out emphasizing the need to scale the data.
    \item Lastly, some features such as mean fractal dimension have values that go from 0.050 to 0.097, and others like the worst area from 185 to 3216, this means that we will have to scale and normalize the data to be able to use it with some classifiers.
\end{itemize}

After looking at the description given by the \textit{describe} method in Table \ref{TAB:ShortStats}, we can see that the data is quite spread out and that we will need to scale it in order to be able to use it in some classifiers.

It is also important to detect the skewness of the data, since it will tell us if the data is symmetric or not. This is important because some classifiers assume that the data is normally distributed, and if it is not, the classifier will not work as well. We can see the skewness of the features in Table \ref{TAB:Skewness}.

\begin{table}[Skewness]{TAB:Skewness}{Skewness of the features in the dataset.}
    \begin{adjustbox}{width=1\linewidth}

        \small
        \begin{tabular}{|c|c|c|c|c|c|}

            \hline
            Mean Radius      & Mean Texture            & Mean Perimeter   & Mean Area              & Mean Smoothness & Mean Compactness        \\

            0.84             & 0.71                    & 0.90             & 1.38                   & 0.51            & 1.22                    \\
            \hline
            Mean Concavity   & Mean Concave Points     & Mean Symmetry    & Mean Fractal Dimension & Radius Error    & Texture Error           \\
            1.44             & 1.14                    & 0.84             & 1.41                   & 1.68            & 1.78                    \\
            \hline
            Perimeter Error  & Area Error              & Smoothness Error & Compactness Error      & Concavity Error & Concave Points Error    \\
            1.85             & 2.15                    & 2.60             & 1.97                   & 5.59            & 1.66                    \\
            \hline
            Symmetry Error   & Fractal Dimension Error & Worst Radius     & Worst Texture          & Worst Perimeter & Worst Area              \\
            2.32             & 4.12                    & 0.95             & 0.44                   & 0.97            & 1.49                    \\
            \hline
            Worst Smoothness & Worst Compactness       & Worst Concavity  & Worst Concave Points   & Worst Symmetry  & Worst Fractal Dimension \\
            0.41             & 1.54                    & 1.24             & 0.46                   & 1.49            & 1.84                    \\
            \hline
            \hline

            \multicolumn{6}{|c|}{Target} \\
            \multicolumn{6}{|c|}{0.52} \\
            \hline
        \end{tabular}
    \end{adjustbox}

\end{table}

The skewness' values can be interpreted as follows:
\begin{itemize}
    \item If the skewness is between -0.5 and 0.5, the data is fairly symmetrical.
    \item If the skewness is above 1 or below -1, the data is highly skewed.
    \item If the skewness is positive, the data is right-skewed.
    \item If the skewness is negative, the data is left-skewed.
\end{itemize}

We will omit the target since it is not a feature but a class. From the values we have obtained in Table \ref{TAB:Skewness}, we can see that all of them are positive, which means that all the features are right-skewed. Some of them are highly skewed with values of up to 5.45 in the case of area error. This unevenness in the distribution is not only a product of the nature of the data, but also because of outliers which we will see in the data visualization section.

\subsection{Data Visualization}

After looking at the descriptive statistics of the data, we can now proceed to visualize the data. The first step is to look at the distribution of the classes, this is important to see if the data is balanced or not, since if the data is not balanced the classifier will have a hard time learning the minority class \cite{he_learning_2009}.

\begin{figure}[Class Distribution]{FIG:ClassDistribution}{Distribution of the classes in the dataset.}
    \image{8cm}{}{class_distribution.png}
\end{figure}

We can see the distribution of the classes in Figure \ref{FIG:ClassDistribution}, where the benign class predominates over the malignant class with 62.56\% of the data being benign and 37.44\% being malignant. Although it is not a 50-50 distribution, it is not a heavily unbalanced dataset where the minority class is less than 20\% of the data, so we will not take any action to balance the data, but we will keep this in mind when we evaluate the classifiers since there could be a bias towards the majority class (benign).

The correlation matrix is the next step in the data visualization, this will allow us to see the relationships between the features, this is important because some classifiers assume that the features are independent, and if they are not, the classifier will not work as well. When looking at the correlation matrix we have to understand that values can range from -1 to 1, where -1 means that the features are negatively correlated, 0 means that the features are not correlated, and 1 means that the features are positively correlated. It is important to study the correlation matrix because if the features are highly correlated it can lead to multicollinearity, which can make the model not work as well \cite{belsley_regression_2005}.

\begin{figure}[Correlation Matrix]{FIG:CorrelationMatrix}{Correlation matrix of the features in the dataset.}
    \image{14cm}{}{correlation_matrix.png}
    % please, play with this number and try to expand it horizontally as much as possible, otherwise it is difficult to read
\end{figure}

In Figure \ref{FIG:CorrelationMatrix} we can see the correlation matrix of the features in the dataset, where we can see that some features are highly correlated with each other. This was expected since some of the features are mathematically related to each other, for example, the area is related to the radius and the perimeter. There is also a tendency for features that are related in the mean to be related in the worst. \ab{these sentences sound strange: 'related in the mean... in the worst', it is better if you could complement the sentence somehow, perhaps adding 'worst measurement' or 'its worst feature value'}

Furthermore, to understand if the data is separable, we will plot the pair plots of the features. For the sake of space, since we have 30 features we will only show the pair plots of the mean features.

\begin{figure}[Pair Plot of the Mean Features]{FIG:PairPlotMean}{Pair plot of the mean features in the dataset.}

    \image{14cm}{}{pair_plot_mean.png}
\end{figure}

In Figure \ref{FIG:PairPlotMean} we can see the pair plot of the mean features in the dataset, where we can see that some features are more separable than others; for example, the mean area and the mean radius are more separable than the mean fractal dimension and the mean symmetry. Along with this, we can also plot the class distribution for each feature to see how separable the classes are. 

\ab{it is strange that sometimes you talk in future tense, when it is something you are 'doing' rightaway... I prefer to have everything in present, but the important thing here is to be consistent and try to use the same tense if possible.}
To see how different the distributions are, we plot the radius in Figure \ref{FIG:ClassDistributionRadius} (since we saw that it was easily separable in the pair plot) and for the fractal dimension in Figure \ref{FIG:ClassDistributionFractal} (since we saw that it was not easily separable in the pair plot). To see the distribution of the other features, we encourage the reader to look at Figures \ref{FIG:FeaturesDistribution1} and \ref{FIG:FeaturesDistribution2} in the Appendix \ref{APP:EDA}.


\begin{figure}[Class Distribution of the Radius]{FIG:ClassDistributionRadius}{Class distribution of the radius in the dataset, from left to right we have the mean, the error, and the worst.}
    \image{10cm}{}{radius_distribution.png}
\end{figure}

\begin{figure}[Class Distribution of the Fractal Dimension]{FIG:ClassDistributionFractal}{Class distribution of the fractal dimension in the dataset, from left to right we have the mean, the error, and the worst.}
    \image{10cm}{}{fractal_distribution.png}
\end{figure}

Now that we know how the data is distributed, we can proceed with the preprocessing. As it was explained in Section \ref{SEC:PREPROCESSING}, we will scale the data using the \textit{StandardScaler} from the \textit{Scikit-learn} library, and we will also perform a \ac{pca} to reduce the dimensionality and collinearity of the data retaining 95\% of the variance, which will result in 10 features made up from a linear combination of the original 30 features. Lastly, this process that we just mentioned will not be done to all the training data, but rather implemented in a pipeline so that for each fold in the cross-validation the data is scaled and transformed using only the training data and not the validation one, to avoid data leakage as already discussed.