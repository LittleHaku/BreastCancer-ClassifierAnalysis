
@misc{njoroge_jean-njorogebreast-cancer-risk-prediction_2024,
	title = {Jean-njoroge/{Breast}-cancer-risk-prediction},
	copyright = {MIT},
	url = {https://github.com/Jean-njoroge/Breast-cancer-risk-prediction},
	abstract = {Classification of Breast Cancer diagnosis Using Support Vector Machines},
	urldate = {2024-01-15},
	author = {Njoroge, Jean-leah},
	month = jan,
	year = {2024},
	note = {original-date: 2016-12-13T04:27:23Z},
	keywords = {breast-cancer-prediction, exploratory-data-analysis, linear-regression, logistic-regression, support-vector-machine, decision-tree, gaussian-naive-bayes, hyperparameter-tuning, knn-classifier, data-preprocessing},
}

@misc{agarap_afagarapwisconsin-breast-cancer_2023,
	title = {{AFAgarap}/wisconsin-breast-cancer},
	copyright = {Apache-2.0},
	url = {https://github.com/AFAgarap/wisconsin-breast-cancer},
	abstract = {[ICMLSC 2018] On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset},
	urldate = {2024-01-15},
	author = {Agarap, Abien Fred},
	month = dec,
	year = {2023},
	note = {original-date: 2017-09-11T15:38:48Z},
	keywords = {breast-cancer-prediction, linear-regression, logistic-regression, machine-learning, multilayer-perceptron, softmax-regression, support-vector-machine, knn-classifier},
}

@misc{bahadur_akshaybahadur21breastcancer_classification_2024,
	title = {akshaybahadur21/{BreastCancer}\_Classification},
	copyright = {MIT},
	url = {https://github.com/akshaybahadur21/BreastCancer_Classification},
	abstract = {Machine learning classifier for cancer tissues üî¨},
	urldate = {2024-01-15},
	author = {Bahadur, Akshay},
	month = jan,
	year = {2024},
	note = {original-date: 2017-10-23T17:00:22Z},
	keywords = {breast-cancer-prediction, logistic-regression, machine-learning},
}

@misc{debbarma_shibajyotidebbarmamachine-learning--scikit-learn-breast-cancer-winconsin-dataset_2023,
	title = {shibajyotidebbarma/{Machine}-{Learning}-with-{Scikit}-{Learn}-{Breast}-{Cancer}-{Winconsin}-{Dataset}},
	url = {https://github.com/shibajyotidebbarma/Machine-Learning-with-Scikit-Learn-Breast-Cancer-Winconsin-Dataset},
	abstract = {In this machine learning project I will work on the Wisconsin Breast Cancer Dataset that comes with scikit-learn. I will train a few algorithms and evaluate their performance. I will use ipython (Jupyter).},
	urldate = {2024-01-15},
	author = {Debbarma, Shibajyoti},
	month = nov,
	year = {2023},
	note = {original-date: 2018-06-23T19:53:32Z},
	keywords = {breast-cancer-prediction, logistic-regression, knn-classifier},
}

@misc{pajo_bora-pajobreast-cancer-prediction_2023,
	title = {bora-pajo/breast-cancer-prediction},
	url = {https://github.com/bora-pajo/breast-cancer-prediction},
	abstract = {Predicting the probability that a diagnosed breast cancer case is malignant or benign based on Wisconsin dataset},
	urldate = {2024-01-15},
	author = {Pajo, Bora},
	month = dec,
	year = {2023},
	note = {original-date: 2017-03-08T21:07:46Z},
	keywords = {logistic-regression, multilayer-perceptron, support-vector-machine, decision-tree, ada-boost, random-forest, quadratic-discriminant-analysis, knn-classifier, naive-bayes-classifier},
}

@article{umer_breast_2022,
	title = {Breast {Cancer} {Detection} {Using} {Convoluted} {Features} and {Ensemble} {Machine} {Learning} {Algorithm}},
	volume = {14},
	issn = {2072-6694},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9737339/},
	doi = {10.3390/cancers14236015},
	abstract = {Simple Summary
This paper presents a breast cancer detection approach where the convoluted features from a convolutional neural network are utilized to train a machine learning model. Results demonstrate that use of convoluted features yields better results than the original features to classify malignant and benign tumors.

Abstract
Breast cancer is a common cause of female mortality in developing countries. Screening and early diagnosis can play an important role in the prevention and treatment of these cancers. This study proposes an ensemble learning-based voting classifier that combines the logistic regression and stochastic gradient descent classifier with deep convoluted features for the accurate detection of cancerous patients. Deep convoluted features are extracted from the microscopic features and fed to the ensemble voting classifier. This idea provides an optimized framework that accurately classifies malignant and benign tumors with improved accuracy. Results obtained using the voting classifier with convoluted features demonstrate that the highest classification accuracy of 100\% is achieved. The proposed approach revealed the accuracy enhancement in comparison with the state-of-the-art approaches.},
	number = {23},
	urldate = {2024-01-16},
	journal = {Cancers},
	author = {Umer, Muhammad and Naveed, Mahum and Alrowais, Fadwa and Ishaq, Abid and Hejaili, Abdullah Al and Alsubai, Shtwai and Eshmawi, Ala‚Äô Abdulmajid and Mohamed, Abdullah and Ashraf, Imran},
	month = dec,
	year = {2022},
	pmid = {36497497},
	pmcid = {PMC9737339},
	keywords = {breast-cancer-prediction, convoluted-features, ensemble-learning, voting-ensemble},
	pages = {6015},
	file = {PubMed Central Full Text PDF:/home/ivan/Zotero/storage/2EEP9XFK/Umer et al. - 2022 - Breast Cancer Detection Using Convoluted Features .pdf:application/pdf},
}

@misc{karanam_exploratory_2022,
	title = {Exploratory {Data} {Analysis} ‚Äî {Breast} {Cancer} {Wisconsin} ({Diagnostic}) {Dataset}},
	url = {https://medium.com/@shashmikaranam/exploratory-data-analysis-breast-cancer-wisconsin-diagnostic-dataset-6a3be9525cd},
	abstract = {Breast Cancer Wisconsin (Diagnostic) Dataset:},
	language = {en},
	urldate = {2024-01-16},
	journal = {Medium},
	author = {Karanam, Shashmi},
	month = sep,
	year = {2022},
	keywords = {exploratory-data-analysis, very-useful},
	file = {Snapshot:/home/ivan/Zotero/storage/6FURYGU3/exploratory-data-analysis-breast-cancer-wisconsin-diagnostic-dataset-6a3be9525cd.html:text/html},
}

@misc{carraro_detanico_breast_2019,
	title = {Breast {Cancer} {Diagnosis} - {EDA} and {Machine} {Learning}},
	url = {https://bdetanico.github.io/Breast-Cancer-Diagnosis/Breast-Cancer-Diagnostic_v5.html},
	abstract = {Exploratory Data Analysis, Preprocessing doing PCA and comparison of KNN, CART (Decision Tree), Random Forest, Logistic Regression and SVM},
	urldate = {2024-01-16},
	author = {Carraro Detanico, Bernardo},
	month = aug,
	year = {2019},
	keywords = {exploratory-data-analysis, logistic-regression, support-vector-machine, decision-tree, random-forest, knn-classifier, principal-component-analysis, data-preprocessing},
}

@article{omotehinwa_light_2023,
	title = {A {Light} {Gradient}-{Boosting} {Machine} algorithm with {Tree}-{Structured} {Parzen} {Estimator} for breast cancer diagnosis},
	volume = {4},
	issn = {2772-4425},
	url = {https://www.sciencedirect.com/science/article/pii/S2772442523000850},
	doi = {10.1016/j.health.2023.100218},
	abstract = {Breast cancer is a common and potentially life-threatening disease. Early and accurate diagnosis of breast cancer is crucial for effective treatment and improved patient outcomes. This study proposed using the Light Gradient-Boosting Machine (LightGBM) algorithm, Borderline- Synthetic Minority Oversampling Technique (SMOTE), and the Tree-Structured Parzen Estimator (TPE) for hyperparameter tuning to enhance the effectiveness of the Machine Learning (ML) model for diagnosing breast cancer. A 10-fold cross-validated TPE optimized Borderline-SMOTE LightGBM classifier was modelled on the Wisconsin Diagnostic Breast Cancer (WDBC) Dataset and evaluated for its performance compared to a baseline LightGBM model. The TPE-optimized Borderline-SMOTE LightGBM model exhibited a significant improvement in performance over the baseline model, achieving an average accuracy of 99.12\%, specificity of 100\%, precision of 100\%, recall of 97.62\%, F1-score of 98.80\%, and a Mathews Correlation Coefficient of 98.12\%. Compared to previous studies, the TPE-optimized Borderline-SMOTE LightGBM model performed exceptionally well. The study demonstrates the effectiveness of using data augmentation and hyperparameter optimization techniques to improve the performance of ML models for breast cancer diagnosis, which has significant implications for the medical field where the accurate and efficient diagnosis of breast cancer is critical.},
	urldate = {2024-01-16},
	journal = {Healthcare Analytics},
	author = {Omotehinwa, Temidayo Oluwatosin and Oyewola, David Opeoluwa and Dada, Emmanuel Gbenga},
	month = dec,
	year = {2023},
	keywords = {breast-cancer-prediction, machine-learning, ensemble-learning, gradient-boosting, hyperparameter-tuning, light-gradient-boosting-machine, tree-structured-parzen-estimator},
	pages = {100218},
	file = {ScienceDirect Snapshot:/home/ivan/Zotero/storage/QPVCJSQU/S2772442523000850.html:text/html},
}

@misc{nighania_various_2019,
	title = {Various ways to evaluate a machine learning models performance},
	url = {https://towardsdatascience.com/various-ways-to-evaluate-a-machine-learning-models-performance-230449055f15},
	abstract = {Because finding accuracy is not enough.},
	language = {en},
	urldate = {2024-01-16},
	journal = {Medium},
	author = {Nighania, Kartik},
	month = jan,
	year = {2019},
	keywords = {machine-learning-evaluation},
	file = {Snapshot:/home/ivan/Zotero/storage/NPPBTKYY/various-ways-to-evaluate-a-machine-learning-models-performance-230449055f15.html:text/html},
}

@misc{choudhury_what_2020,
	title = {What is {Gradient} {Boosting}? {How} is it different from {Ada} {Boost}?},
	shorttitle = {What is {Gradient} {Boosting}?},
	url = {https://medium.com/analytics-vidhya/what-is-gradient-boosting-how-is-it-different-from-ada-boost-2d5ff5767cb2},
	abstract = {Boosting algorithms are one of the most popular and used algorithms. They can be considered as one of the most powerful techniques for‚Ä¶},
	language = {en},
	urldate = {2024-01-16},
	journal = {Analytics Vidhya},
	author = {Choudhury, Abhiroop},
	month = dec,
	year = {2020},
	keywords = {ada-boost, ensemble-learning, gradient-boosting},
	file = {Snapshot:/home/ivan/Zotero/storage/4WZCR947/what-is-gradient-boosting-how-is-it-different-from-ada-boost-2d5ff5767cb2.html:text/html},
}

@misc{scikit_learn_1_nodate,
	title = {1. {Supervised} learning},
	url = {https://scikit-learn/stable/supervised_learning.html},
	abstract = {Linear Models- Ordinary Least Squares, Ridge regression and classification, Lasso, Multi-task Lasso, Elastic-Net, Multi-task Elastic-Net, Least Angle Regression, LARS Lasso, Orthogonal Matching Pur...},
	language = {en},
	urldate = {2024-01-16},
	journal = {scikit-learn},
	author = {{Scikit Learn}},
	file = {Snapshot:/home/ivan/Zotero/storage/2X2QGMLT/supervised_learning.html:text/html},
}

@inproceedings{sharif_moghadam_breast_2019,
	title = {Breast {Cancer} {Classification} {Using} {AdaBoost}- {Extreme} {Learning} {Machine}},
	doi = {10.1109/ICSPIS48872.2019.9066088},
	author = {Sharif Moghadam, Mahboobe and Jazayeriy, Hamid},
	month = dec,
	year = {2019},
	keywords = {breast-cancer-prediction, ada-boost},
	pages = {1--5},
	file = {Full Text PDF:/home/ivan/Zotero/storage/G3IFDQR2/Sharif Moghadam and Jazayeriy - 2019 - Breast Cancer Classification Using AdaBoost- Extre.pdf:application/pdf},
}

@article{minnoor_diagnosis_2023,
	series = {International {Conference} on {Machine} {Learning} and {Data} {Engineering}},
	title = {Diagnosis of {Breast} {Cancer} {Using} {Random} {Forests}},
	volume = {218},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S187705092300025X},
	doi = {10.1016/j.procs.2023.01.025},
	abstract = {Breast cancer was the most diagnosed form of cancer in 2020. Early diagnosis of breast cancer results in a significant improvement in long-term survival rates. Current methods require consultation with experts, which is expensive and time-consuming and thus may not be accessible to all. This paper seeks to train and evaluate supervised machine learning models for the accurate and efficient detection of breast cancer. The Wisconsin Breast Cancer Database dataset describes 30 attributes of cell nuclei, including, but not limited to, their radius, texture, and concavity. It contains 569 instances, 212 of which are malignant tumors. The Random Forest algorithm outperforms other algorithms in classifying breast tumors as either malignant or benign and is thus selected as our primary model. It is trained on two different subsets of the dataset having 16 and 8 features, respectively, identified with the help of multiple feature selection methods. The Random Forest models are tested post hyperparameter tuning on a holdout set, and accuracies of 100\% and 99.30\% respectively. The models are also compared with four other machine learning classification algorithms: Support Vector Machine (SVM), Decision Tree, Multilayer Perceptron, and K-Nearest Neighbors. The results confirm that Random Forest is the superior method for breast cancer diagnosis.},
	urldate = {2024-01-16},
	journal = {Procedia Computer Science},
	author = {Minnoor, Manas and Baths, Veeky},
	month = jan,
	year = {2023},
	keywords = {breast-cancer-prediction, random-forest, hyperparameter-tuning},
	pages = {429--437},
	file = {ScienceDirect Snapshot:/home/ivan/Zotero/storage/AIJLAGHL/S187705092300025X.html:text/html},
}

@article{khatun_cancer_2023,
	title = {Cancer {Classification} {Utilizing} {Voting} {Classifier} with {Ensemble} {Feature} {Selection} {Method} and {Transcriptomic} {Data}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4425},
	url = {https://www.mdpi.com/2073-4425/14/9/1802},
	doi = {10.3390/genes14091802},
	abstract = {Biomarker-based cancer identification and classification tools are widely used in bioinformatics and machine learning fields. However, the high dimensionality of microarray gene expression data poses a challenge for identifying important genes in cancer diagnosis. Many feature selection algorithms optimize cancer diagnosis by selecting optimal features. This article proposes an ensemble rank-based feature selection method (EFSM) and an ensemble weighted average voting classifier (VT) to overcome this challenge. The EFSM uses a ranking method that aggregates features from individual selection methods to efficiently discover the most relevant and useful features. The VT combines support vector machine, k-nearest neighbor, and decision tree algorithms to create an ensemble model. The proposed method was tested on three benchmark datasets and compared to existing built-in ensemble models. The results show that our model achieved higher accuracy, with 100\% for leukaemia, 94.74\% for colon cancer, and 94.34\% for the 11-tumor dataset. This study concludes by identifying a subset of the most important cancer-causing genes and demonstrating their significance compared to the original data. The proposed approach surpasses existing strategies in accuracy and stability, significantly impacting the development of ML-based gene analysis. It detects vital genes with higher precision and stability than other existing methods.},
	language = {en},
	number = {9},
	urldate = {2024-01-16},
	journal = {Genes},
	author = {Khatun, Rabea and Akter, Maksuda and Islam, Md Manowarul and Uddin, Md Ashraf and Talukder, Md Alamin and Kamruzzaman, Joarder and Azad, A. K. M. and Paul, Bikash Kumar and Almoyad, Muhammad Ali Abdulllah and Aryal, Sunil and Moni, Mohammad Ali},
	month = sep,
	year = {2023},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {breast-cancer-prediction, machine-learning, ensemble-learning, voting-ensemble, feature-selection},
	pages = {1802},
	file = {Full Text PDF:/home/ivan/Zotero/storage/757L6J7T/Khatun et al. - 2023 - Cancer Classification Utilizing Voting Classifier .pdf:application/pdf},
}

@misc{njoroge_breast-cancer-risk-predictionnb2_exploratorydataanalysisipynb_nodate,
	title = {Breast-cancer-risk-prediction/{NB2}\_ExploratoryDataAnalysis.ipynb at master ¬∑ {Jean}-njoroge/{Breast}-cancer-risk-prediction},
	url = {https://github.com/Jean-njoroge/Breast-cancer-risk-prediction/blob/master/NB2_ExploratoryDataAnalysis.ipynb},
	abstract = {Classification of Breast Cancer diagnosis Using Support Vector Machines - Jean-njoroge/Breast-cancer-risk-prediction},
	language = {en},
	urldate = {2024-01-16},
	journal = {GitHub},
	author = {Njoroge, Jean-leah},
	keywords = {exploratory-data-analysis},
	file = {Snapshot:/home/ivan/Zotero/storage/THEKV93F/NB2_ExploratoryDataAnalysis.html:text/html},
}

@misc{wcrf_international_breast_nodate,
	title = {Breast cancer statistics {\textbar} {World} {Cancer} {Research} {Fund} {International}},
	url = {https://www.wcrf.org/cancer-trends/breast-cancer-statistics/},
	abstract = {Latest statistics on breast cancer, with data on which countries have the highest rates, plus advice on preventing breast cancer.},
	language = {en-US},
	urldate = {2024-01-16},
	journal = {WCRF International},
	author = {{WCRF International}},
	keywords = {breast-cancer-statistics, motivation},
	file = {Snapshot:/home/ivan/Zotero/storage/DIZTPLS9/breast-cancer-statistics.html:text/html},
}

@misc{american_cancer_society_breast_nodate,
	title = {Breast {Cancer} {Statistics} {\textbar} {How} {Common} {Is} {Breast} {Cancer}?},
	url = {https://www.cancer.org/cancer/types/breast-cancer/about/how-common-is-breast-cancer.html},
	abstract = {Read the American Cancer Society‚Äôs latest information and statistics for breast cancer in women in the United States.},
	language = {en},
	urldate = {2024-01-16},
	author = {{American Cancer Society}},
	keywords = {breast-cancer-statistics, motivation},
	file = {Snapshot:/home/ivan/Zotero/storage/P9WCXHYZ/how-common-is-breast-cancer.html:text/html},
}

@article{yu_diagnostic_2012,
	title = {Diagnostic value of fine-needle aspiration biopsy for breast mass: a systematic review and meta-analysis},
	volume = {12},
	issn = {1471-2407},
	shorttitle = {Diagnostic value of fine-needle aspiration biopsy for breast mass},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3283452/},
	doi = {10.1186/1471-2407-12-41},
	abstract = {Background
Fine-needle aspiration biopsy (FNAB) of the breast is a minimally invasive yet maximally diagnostic method. However, the clinical use of FNAB has been questioned. The purpose of our study was to establish the overall value of FNAC in the diagnosis of breast lesions.

Methods
After a review and quality assessment of 46 studies, sensitivity, specificity and other measures of accuracy of FNAB for evaluating breast lesions were pooled using random-effects models. Summary receiver operating characteristic curves were used to summarize overall accuracy. The sensitivity and specificity for the studies data (included unsatisfactory samples) and underestimation rate of unsatisfactory samples were also calculated.

Results
The summary estimates for FNAB in diagnosis of breast carcinoma were as follows (unsatisfactory samples was temporarily exluded): sensitivity, 0.927 (95\% confidence interval [CI], 0.921 to 0.933); specificity, 0.948 (95\% CI, 0.943 to 0.952); positive likelihood ratio, 25.72 (95\% CI, 17.35 to 28.13); negative likelihood ratio, 0.08 (95\% CI, 0.06 to 0.11); diagnostic odds ratio, 429.73 (95\% CI, 241.75 to 763.87); The pooled sensitivity and specificity for 11 studies, which reported unsatisfactory samples (unsatisfactory samples was considered to be positive in this classification) were 0.920 (95\% CI, 0.906 to 0.933) and 0.768 (95\% CI, 0.751 to 0.784) respectively. The pooled proportion of unsatisfactory samples that were subsequently upgraded to various grade cancers was 27.5\% (95\% CI, 0.221 to 0.296).

Conclusions
FNAB is an accurate biopsy for evaluating breast malignancy if rigorous criteria are used. With regard to unsatisfactory samples, futher invasive procedures are required in order to minimize the chance of a missed diagnosis of breast cancer.},
	urldate = {2024-01-16},
	journal = {BMC Cancer},
	author = {Yu, Ying-Hua and Wei, Wei and Liu, Jian-Lun},
	month = jan,
	year = {2012},
	pmid = {22277164},
	pmcid = {PMC3283452},
	keywords = {breast-cancer-statistics, motivation, FNAB-statistics},
	pages = {41},
	file = {PubMed Central Full Text PDF:/home/ivan/Zotero/storage/MSRA3ISW/Yu et al. - 2012 - Diagnostic value of fine-needle aspiration biopsy .pdf:application/pdf},
}

@misc{hall_gecko17kmachinelearningworkflow_2020,
	title = {gecko17k/{MachineLearningWorkflow}},
	url = {https://github.com/gecko17k/MachineLearningWorkflow},
	abstract = {Using breast cancer data to demonstrate a good Machine Learning workflow: loading data, cleaning it, standardising it, comparing different ML models, improving the best, presenting the results.},
	urldate = {2024-01-16},
	author = {Hall, Vincent},
	month = nov,
	year = {2020},
	note = {original-date: 2019-07-22T08:30:32Z},
	keywords = {breast-cancer-prediction, exploratory-data-analysis, logistic-regression, multilayer-perceptron, support-vector-machine, decision-tree, gaussian-naive-bayes, quadratic-discriminant-analysis, knn-classifier, data-preprocessing, linear-discriminant-analysis, very-useful},
}

@misc{baeldung_how_2021,
	title = {How {Many} {Principal} {Components} to {Take} in {PCA}? {\textbar} {Baeldung} on {Computer} {Science}},
	shorttitle = {How {Many} {Principal} {Components} to {Take} in {PCA}?},
	url = {https://www.baeldung.com/cs/pca},
	abstract = {Explore the Principal Component Analysis.},
	language = {en-US},
	urldate = {2024-01-18},
	author = {{baeldung}},
	month = oct,
	year = {2021},
	keywords = {principal-component-analysis},
	file = {Snapshot:/home/ivan/Zotero/storage/JHDNQ5SE/pca.html:text/html},
}

@misc{zhao_pre-process_2019,
	title = {Pre-{Process} {Data} with {Pipeline} to {Prevent} {Data} {Leakage} during {Cross}-{Validation}.},
	url = {https://towardsdatascience.com/pre-process-data-with-pipeline-to-prevent-data-leakage-during-cross-validation-e3442cca7fdc},
	abstract = {In machine learning, K-fold Cross-validation is a frequently used validation technique for assessing how the results of a statistical‚Ä¶},
	language = {en},
	urldate = {2024-01-18},
	journal = {Medium},
	author = {Zhao, Kai},
	month = sep,
	year = {2019},
	keywords = {hyperparameter-tuning, data-preprocessing, cross-validation},
	file = {Snapshot:/home/ivan/Zotero/storage/6TQ6V7TN/pre-process-data-with-pipeline-to-prevent-data-leakage-during-cross-validation-e3442cca7fdc.html:text/html},
}

@misc{pramoditha_how_2023,
	title = {How to {Select} the {Best} {Number} of {Principal} {Components} for the {Dataset}},
	url = {https://towardsdatascience.com/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d},
	abstract = {Six methods you should follow},
	language = {en},
	urldate = {2024-01-18},
	journal = {Medium},
	author = {Pramoditha, Rukshan},
	month = aug,
	year = {2023},
	keywords = {principal-component-analysis},
	file = {Snapshot:/home/ivan/Zotero/storage/23CJTRMQ/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d.html:text/html},
}

@misc{scikit_learn_31_nodate,
	title = {3.1. {Cross}-validation: evaluating estimator performance},
	shorttitle = {3.1. {Cross}-validation},
	url = {https://scikit-learn/stable/modules/cross_validation.html},
	abstract = {Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would ha...},
	language = {en},
	urldate = {2024-01-19},
	journal = {scikit-learn},
	author = {{Scikit Learn}},
	keywords = {cross-validation},
	file = {Snapshot:/home/ivan/Zotero/storage/AJURQ6WZ/cross_validation.html:text/html},
}

@misc{mwiti_comprehensive_2022,
	title = {A {Comprehensive} {Guide} to {Ensemble} {Learning}: {What} {Exactly} {Do} {You} {Need} to {Know}},
	shorttitle = {A {Comprehensive} {Guide} to {Ensemble} {Learning}},
	url = {https://neptune.ai/blog/ensemble-learning-guide},
	abstract = {Explore ensemble learning methods, libraries for stacking, and optimal use-cases in a straightforward guide.},
	language = {en-US},
	urldate = {2024-01-19},
	journal = {neptune.ai},
	author = {Mwiti, Derrick},
	month = jul,
	year = {2022},
	keywords = {ensemble-learning},
	file = {Snapshot:/home/ivan/Zotero/storage/9V2ABLR2/ensemble-learning-guide.html:text/html},
}

@misc{srivastava_powerful_2015,
	title = {Powerful '{Trick}' to choose right models in {Ensemble} {Learning}},
	url = {https://www.analyticsvidhya.com/blog/2015/10/trick-right-model-ensemble/},
	abstract = {Ensemble modeling boost the power of predictive model. This trick to select the right model helps to build accurate machine learning models},
	language = {en},
	urldate = {2024-01-19},
	journal = {Analytics Vidhya},
	author = {Srivastava, Tavish},
	month = oct,
	year = {2015},
	keywords = {ensemble-learning, very-useful},
	file = {Snapshot:/home/ivan/Zotero/storage/Y24EMGEH/trick-right-model-ensemble.html:text/html},
}

@misc{nantasenamat_codepythoncomparing-classifiersipynb_nodate,
	title = {code/python/comparing-classifiers.ipynb at master ¬∑ dataprofessor/code},
	url = {https://github.com/dataprofessor/code/blob/master/python/comparing-classifiers.ipynb},
	abstract = {Compilation of R and Python programming codes on the Data Professor YouTube channel. - dataprofessor/code},
	language = {en},
	urldate = {2024-01-19},
	journal = {GitHub},
	author = {Nantasenamat, Chanin},
	keywords = {compare-classifiers},
	file = {Snapshot:/home/ivan/Zotero/storage/T5YND5BW/comparing-classifiers.html:text/html},
}

@misc{pandian_comprehensive_2022,
	title = {A {Comprehensive} {Guide} on {Hyperparameter} {Tuning} and its {Techniques}},
	url = {https://www.analyticsvidhya.com/blog/2022/02/a-comprehensive-guide-on-hyperparameter-tuning-and-its-techniques/},
	abstract = {Every Data Scientist must understand the significance of hyperparameter tuning techniques while selecting right Machine Learning model.},
	language = {en},
	urldate = {2024-01-19},
	journal = {Analytics Vidhya},
	author = {Pandian, Shanthababu},
	month = feb,
	year = {2022},
	keywords = {hyperparameter-tuning},
	file = {Snapshot:/home/ivan/Zotero/storage/9JAZ7NKS/a-comprehensive-guide-on-hyperparameter-tuning-and-its-techniques.html:text/html},
}

@misc{firebug_answer_2016,
	title = {Answer to "{Practical} hyperparameter optimization: {Random} vs. grid search"},
	shorttitle = {Answer to "{Practical} hyperparameter optimization},
	url = {https://stats.stackexchange.com/a/209409},
	urldate = {2024-01-20},
	journal = {Cross Validated},
	author = {Firebug},
	month = apr,
	year = {2016},
	keywords = {hyperparameter-tuning, random-search-cv},
	file = {Snapshot:/home/ivan/Zotero/storage/YYZB8285/practical-hyperparameter-optimization-random-vs-grid-search.html:text/html},
}

@misc{ismiguzel_hyperparameter_2023,
	title = {Hyperparameter {Tuning} with {Grid} {Search} and {Random} {Search}},
	url = {https://towardsdatascience.com/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144},
	abstract = {And a deep dive into how to combine them},
	language = {en},
	urldate = {2024-01-20},
	journal = {Medium},
	author = {Ismiguzel, Idil},
	month = mar,
	year = {2023},
	keywords = {hyperparameter-tuning, random-search-cv, grid-search-cv},
	file = {Snapshot:/home/ivan/Zotero/storage/FCFVM9ZU/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144.html:text/html},
}

@misc{boyle_hyperparameter_nodate,
	title = {Hyperparameter {Tuning}},
	url = {https://kaggle.com/code/tboyle10/hyperparameter-tuning},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Don't Overfit! II},
	language = {en},
	urldate = {2024-01-20},
	author = {Boyle, Tara},
	keywords = {hyperparameter-tuning, sgd-classifier},
	file = {Snapshot:/home/ivan/Zotero/storage/GASBN7RP/hyperparameter-tuning.html:text/html},
}

@misc{tiwari_advanced_2023,
	title = {Advanced {Evaluation} {Metrics} for {Imbalanced} {Classification} {Models}},
	url = {https://medium.com/cuenex/advanced-evaluation-metrics-for-imbalanced-classification-models-ee6f248c90ca},
	abstract = {Measure Imbalanced Models the right way!},
	language = {en},
	urldate = {2024-01-25},
	journal = {CueNex},
	author = {Tiwari, Rajneesh},
	month = feb,
	year = {2023},
	keywords = {compare-classifiers, evaluation-metrics},
	file = {Snapshot:/home/ivan/Zotero/storage/W76ATLNU/advanced-evaluation-metrics-for-imbalanced-classification-models-ee6f248c90ca.html:text/html},
}

@misc{rutecki_best_nodate,
	title = {Best techniques and metrics for {Imbalanced} {Dataset}},
	url = {https://kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from multiple data sources},
	language = {en},
	urldate = {2024-01-25},
	author = {Rutecki, Marcin},
	keywords = {compare-classifiers, evaluation-metrics},
	file = {Snapshot:/home/ivan/Zotero/storage/V59GWJZZ/best-techniques-and-metrics-for-imbalanced-dataset.html:text/html},
}

@misc{rutecki_voting_nodate,
	title = {Voting {Classifier} for {Better} {Results}},
	url = {https://kaggle.com/code/marcinrutecki/voting-classifier-for-better-results},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from multiple data sources},
	language = {en},
	urldate = {2024-01-25},
	author = {Rutecki, Marcin},
	keywords = {voting-ensemble},
	file = {Snapshot:/home/ivan/Zotero/storage/7NZJ86PL/voting-classifier-for-better-results.html:text/html},
}

@misc{kawa_explainable_2022,
	title = {Explainable {AI} ({XAI}) with {Class} {Maps}},
	url = {https://towardsdatascience.com/explainable-ai-xai-with-class-maps-d0e137a91d2c},
	abstract = {Introducing a novel visual tool for explaining the results of classification algorithms, with examples in R and Python.},
	language = {en},
	urldate = {2024-01-29},
	journal = {Medium},
	author = {Kawa, Nura},
	month = feb,
	year = {2022},
	keywords = {explainable-ai},
	file = {Snapshot:/home/ivan/Zotero/storage/NP28NKTS/explainable-ai-xai-with-class-maps-d0e137a91d2c.html:text/html},
}

@article{dindorf_classification_2021,
	title = {Classification and {Automated} {Interpretation} of {Spinal} {Posture} {Data} {Using} a {Pathology}-{Independent} {Classifier} and {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/18/6323},
	doi = {10.3390/s21186323},
	abstract = {Clinical classification models are mostly pathology-dependent and, thus, are only able to detect pathologies they have been trained for. Research is needed regarding pathology-independent classifiers and their interpretation. Hence, our aim is to develop a pathology-independent classifier that provides prediction probabilities and explanations of the classification decisions. Spinal posture data of healthy subjects and various pathologies (back pain, spinal fusion, osteoarthritis), as well as synthetic data, were used for modeling. A one-class support vector machine was used as a pathology-independent classifier. The outputs were transformed into a probability distribution according to Platt‚Äôs method. Interpretation was performed using the explainable artificial intelligence tool Local Interpretable Model-Agnostic Explanations. The results were compared with those obtained by commonly used binary classification approaches. The best classification results were obtained for subjects with a spinal fusion. Subjects with back pain were especially challenging to distinguish from the healthy reference group. The proposed method proved useful for the interpretation of the predictions. No clear inferiority of the proposed approach compared to commonly used binary classifiers was demonstrated. The application of dynamic spinal data seems important for future works. The proposed approach could be useful to provide an objective orientation and to individually adapt and monitor therapy measures pre- and post-operatively.},
	language = {en},
	number = {18},
	urldate = {2024-01-29},
	journal = {Sensors},
	author = {Dindorf, Carlo and Konradi, J√ºrgen and Wolf, Claudia and Taetz, Bertram and Bleser, Gabriele and Huthwelker, Janine and Werthmann, Friederike and Bartaguiz, Eva and Kniepert, Johanna and Drees, Philipp and Betz, Ulrich and Fr√∂hlich, Michael},
	month = jan,
	year = {2021},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {explainable-ai, LIME},
	pages = {6323},
	file = {Full Text PDF:/home/ivan/Zotero/storage/IE9CWKM3/Dindorf et al. - 2021 - Classification and Automated Interpretation of Spi.pdf:application/pdf},
}

@article{massafra_analyzing_2023,
	title = {Analyzing breast cancer invasive disease event classification through explainable artificial intelligence},
	volume = {10},
	issn = {2296-858X},
	url = {https://www.frontiersin.org/articles/10.3389/fmed.2023.1116354},
	abstract = {IntroductionRecently, accurate machine learning and deep learning approaches have been dedicated to the investigation of breast cancer invasive disease events (IDEs), such as recurrence, contralateral and second cancers. However, such approaches are poorly interpretable.MethodsThus, we designed an Explainable Artificial Intelligence (XAI) framework to investigate IDEs within a cohort of 486 breast cancer patients enrolled at IRCCS Istituto Tumori ‚ÄúGiovanni Paolo II‚Äù in Bari, Italy. Using Shapley values, we determined the IDE driving features according to two periods, often adopted in clinical practice, of 5 and 10 years from the first tumor diagnosis.ResultsAge, tumor diameter, surgery type, and multiplicity are predominant within the 5-year frame, while therapy-related features, including hormone, chemotherapy schemes and lymphovascular invasion, dominate the 10-year IDE prediction. Estrogen Receptor (ER), proliferation marker Ki67 and metastatic lymph nodes affect both frames.DiscussionThus, our framework aims at shortening the distance between AI and clinical practice},
	urldate = {2024-01-29},
	journal = {Frontiers in Medicine},
	author = {Massafra, Raffaella and Fanizzi, Annarita and Amoroso, Nicola and Bove, Samantha and Comes, Maria Colomba and Pomarico, Domenico and Didonna, Vittorio and Diotaiuti, Sergio and Galati, Luisa and Giotta, Francesco and La Forgia, Daniele and Latorre, Agnese and Lombardi, Angela and Nardone, Annalisa and Pastena, Maria Irene and Ressa, Cosmo Maurizio and Rinaldi, Lucia and Tamborra, Pasquale and Zito, Alfredo and Paradiso, Angelo Virgilio and Bellotti, Roberto and Lorusso, Vito},
	year = {2023},
	keywords = {explainable-ai},
	file = {Full Text PDF:/home/ivan/Zotero/storage/RTG3FJBN/Massafra et al. - 2023 - Analyzing breast cancer invasive disease event cla.pdf:application/pdf},
}

@misc{cohen_explainable_2022,
	title = {Explainable {AI} ({XAI}) with {SHAP} -{Multi}-class classification problem},
	url = {https://towardsdatascience.com/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea},
	abstract = {Practical guide for XAI analysis with SHAP for a Multi-class classification problem},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Cohen, Idit},
	month = may,
	year = {2022},
	keywords = {explainable-ai, SHAP},
	file = {Snapshot:/home/ivan/Zotero/storage/LCTVKUPP/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea.html:text/html},
}

@misc{gupta_classification_nodate,
	title = {Classification {Feature} {Selection} : {SHAP} {Tutorial}},
	shorttitle = {Classification {Feature} {Selection}},
	url = {https://kaggle.com/code/ritzig/classification-feature-selection-shap-tutorial},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from Mobile Price Classification},
	language = {en},
	urldate = {2024-02-01},
	author = {Gupta, Ritika},
	keywords = {explainable-ai, SHAP},
	file = {Snapshot:/home/ivan/Zotero/storage/8GTCHX25/notebook.html:text/html},
}

@article{gramegna_shap_2021,
	title = {{SHAP} and {LIME}: {An} {Evaluation} of {Discriminative} {Power} in {Credit} {Risk}},
	volume = {4},
	issn = {2624-8212},
	shorttitle = {{SHAP} and {LIME}},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2021.752558},
	abstract = {In credit risk estimation, the most important element is obtaining a probability of default as close as possible to the effective risk. This effort quickly prompted new, powerful algorithms that reach a far higher accuracy, but at the cost of losing intelligibility, such as Gradient Boosting or ensemble methods. These models are usually referred to as ‚Äúblack-boxes‚Äù, implying that you know the inputs and the output, but there is little way to understand what is going on under the hood. As a response to that, we have seen several different Explainable AI models flourish in recent years, with the aim of letting the user see why the black-box gave a certain output. In this context, we evaluate two very popular eXplainable AI (XAI) models in their ability to discriminate observations into groups, through the application of both unsupervised and predictive modeling to the weights these XAI models assign to features locally. The evaluation is carried out on real Small and Medium Enterprises data, obtained from official italian repositories, and may form the basis for the employment of such XAI models for post-processing features extraction.},
	urldate = {2024-02-01},
	journal = {Frontiers in Artificial Intelligence},
	author = {Gramegna, Alex and Giudici, Paolo},
	year = {2021},
	keywords = {explainable-ai, LIME, SHAP},
	file = {Full Text PDF:/home/ivan/Zotero/storage/PHGRIG2E/Gramegna and Giudici - 2021 - SHAP and LIME An Evaluation of Discriminative Pow.pdf:application/pdf},
}

@misc{radecic_shap_2022,
	title = {{SHAP}: {How} to {Interpret} {Machine} {Learning} {Models} {With} {Python}},
	shorttitle = {{SHAP}},
	url = {https://towardsdatascience.com/shap-how-to-interpret-machine-learning-models-with-python-2323f5af4be9},
	abstract = {Explainable machine learning with a single function call},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Radeƒçiƒá, Dario},
	month = mar,
	year = {2022},
	keywords = {explainable-ai, SHAP},
	file = {Snapshot:/home/ivan/Zotero/storage/EA4J7PK6/shap-how-to-interpret-machine-learning-models-with-python-2323f5af4be9.html:text/html},
}

@misc{radecic_lime_2022,
	title = {{LIME}: {How} to {Interpret} {Machine} {Learning} {Models} {With} {Python}},
	shorttitle = {{LIME}},
	url = {https://towardsdatascience.com/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e},
	abstract = {Explainable machine learning at your fingertips.},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Radeƒçiƒá, Dario},
	month = mar,
	year = {2022},
	keywords = {explainable-ai, LIME},
	file = {Snapshot:/home/ivan/Zotero/storage/3MLHPB6K/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e.html:text/html},
}

@misc{radecic_lime_2022-1,
	title = {{LIME} vs. {SHAP}: {Which} is {Better} for {Explaining} {Machine} {Learning} {Models}?},
	shorttitle = {{LIME} vs. {SHAP}},
	url = {https://towardsdatascience.com/lime-vs-shap-which-is-better-for-explaining-machine-learning-models-d68d8290bb16},
	abstract = {Two of the most popular Explainers compared.},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Radeƒçiƒá, Dario},
	month = mar,
	year = {2022},
	keywords = {explainable-ai, LIME, SHAP},
	file = {Snapshot:/home/ivan/Zotero/storage/AMSD6WBC/lime-vs-shap-which-is-better-for-explaining-machine-learning-models-d68d8290bb16.html:text/html},
}

@misc{luvsandorj_explaining_2021,
	title = {Explaining {Scikit}-learn models with {SHAP}},
	url = {https://towardsdatascience.com/explaining-scikit-learn-models-with-shap-61daff21b12a},
	abstract = {Towards explainable AI},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Luvsandorj, Zolzaya},
	month = nov,
	year = {2021},
	keywords = {explainable-ai, SHAP},
	file = {Snapshot:/home/ivan/Zotero/storage/7SPHVCD5/explaining-scikit-learn-models-with-shap-61daff21b12a.html:text/html},
}

@article{de_lange_explainable_2022,
	title = {Explainable {AI} for {Credit} {Assessment} in {Banks}},
	volume = {15},
	doi = {10.3390/jrfm15120556},
	abstract = {Banks‚Äô credit scoring models are required by financial authorities to be explainable. This paper proposes an explainable artificial intelligence (XAI) model for predicting credit default on a unique dataset of unsecured consumer loans provided by a Norwegian bank. We combined a LightGBM model with SHAP, which enables the interpretation of explanatory variables affecting the predictions. The LightGBM model clearly outperforms the bank‚Äôs actual credit scoring model (Logistic Regression). We found that the most important explanatory variables for predicting default in the LightGBM model are the volatility of utilized credit balance, remaining credit in percentage of total credit and the duration of the customer relationship. Our main contribution is the implementation of XAI methods in banking, exploring how these methods can be applied to improve the interpretability and reliability of state-of-the-art AI models. We also suggest a method for analyzing the potential economic value of an improved credit scoring model.},
	journal = {Journal of Risk and Financial Management},
	author = {de Lange, Petter and Melsom, Borger and Venner√∏d, Christian and Westgaard, Sjur},
	month = nov,
	year = {2022},
	keywords = {explainable-ai, SHAP},
	pages = {556},
	file = {Full Text:/home/ivan/Zotero/storage/7XE2S6XQ/de Lange et al. - 2022 - Explainable AI for Credit Assessment in Banks.pdf:application/pdf},
}

@misc{spanhol_figure_nodate,
	title = {Figure 1. {A} slide of breast malignant tumor (stained with {HE}) seen in...},
	url = {https://www.researchgate.net/figure/A-slide-of-breast-malignant-tumor-stained-with-HE-seen-in-different-magnification_fig1_304158394},
	abstract = {Download scientific diagram {\textbar} A slide of breast malignant tumor (stained with HE) seen in different magnification factors: (a) 40√ó, (b) 100√ó, (c) 200√ó, and (d) 400√ó. Highlighted rectangle (manually added for illustrative purposes only) is the area of interest selected by pathologist to be detailed in the next higher magnification factor. ¬† from publication: Breast Cancer Histopathological Image Classification using Convolutional Neural Networks {\textbar} The performance of most conventional classification systems relies on appropriate data representation and much of the efforts are dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful... {\textbar} Image Classification, Convolution and Breast Cancer {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-02-19},
	journal = {ResearchGate},
	author = {Spanhol, Fabio},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/P4Z9DNWT/A-slide-of-breast-malignant-tumor-stained-with-HE-seen-in-different-magnification_fig1_30415839.html:text/html},
}

@misc{william_wolberg_breast_1993,
	title = {Breast {Cancer} {Wisconsin} ({Diagnostic})},
	url = {https://archive.ics.uci.edu/dataset/17},
	doi = {10.24432/C5DW2B},
	urldate = {2024-02-27},
	publisher = {UCI Machine Learning Repository},
	author = {William Wolberg, Olvi Mangasarian},
	year = {1993},
	keywords = {dataset},
}

@article{sidey-gibbons_machine_2019,
	title = {Machine learning in medicine: a practical introduction},
	volume = {19},
	copyright = {2019 The Author(s)},
	issn = {1471-2288},
	shorttitle = {Machine learning in medicine},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0681-4},
	doi = {10.1186/s12874-019-0681-4},
	abstract = {Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. The trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble. We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.},
	language = {en},
	number = {1},
	urldate = {2024-02-27},
	journal = {BMC Medical Research Methodology},
	author = {Sidey-Gibbons, Jenni A. M. and Sidey-Gibbons, Chris J.},
	month = dec,
	year = {2019},
	note = {Number: 1
Publisher: BioMed Central},
	keywords = {image},
	pages = {1--18},
	file = {Full Text PDF:/home/ivan/Zotero/storage/F2H8USH2/Sidey-Gibbons and Sidey-Gibbons - 2019 - Machine learning in medicine a practical introduc.pdf:application/pdf},
}

@article{daffertshofer_pca_2004,
	title = {{PCA} in studying coordination and variability: a tutorial},
	volume = {19},
	issn = {02680033},
	shorttitle = {{PCA} in studying coordination and variability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0268003304000166},
	doi = {10.1016/j.clinbiomech.2004.01.005},
	abstract = {Objective. To explain and underscore the use of principal component analysis in clinical biomechanics as an expedient, unbiased means for reducing high-dimensional data sets to a small number of modes or structures, as well as for teasing apart structural (invariant) and variable components in such data sets. Design. The method is explained formally and then applied to both simulated and real (kinematic and electromyographic) data for didactical purposes, thus illustrating possible applications (and pitfalls) in the study of coordinated movement.
Background. In the sciences at large, principal component analysis is a well-known method to remove redundant information in multidimensional data sets by means of mode reduction. At present, principal component analysis is starting to penetrate the fundamental and clinical study of human movement, which ampliÔ¨Åes the need for an accessible explanation of the method and its possibilities and limitations. Besides mode reduction, we discuss principal component analysis in its capacity as a data-driven Ô¨Ålter, allowing for a separation of invariant and variant properties of coordination, which, arguably, is essential in studies of motor variability.
Methods. Principal component analysis is applied to kinematic and electromyographic time series obtained during treadmill walking by healthy humans.
Results. Common signal structures or modes are identiÔ¨Åed in the time series that turn out to be readily interpretable. In addition, the identiÔ¨Åed coherent modes are eliminated from the data, leaving a Ô¨Åltered, residual pattern from which useful information may be gleaned regarding motor variability.
Conclusions. Principal component analysis allows for the detection of modes (information reduction) in both kinematic and electromyographic data sets, as well as for the separation of invariant structure and variance in those data sets.},
	language = {en},
	number = {4},
	urldate = {2024-03-01},
	journal = {Clinical Biomechanics},
	author = {Daffertshofer, Andreas and Lamoth, Claudine J.C. and Meijer, Onno G. and Beek, Peter J.},
	month = may,
	year = {2004},
	keywords = {principal-component-analysis},
	pages = {415--428},
	file = {Daffertshofer et al. - 2004 - PCA in studying coordination and variability a tu.pdf:/home/ivan/Zotero/storage/ILH8G8CL/Daffertshofer et al. - 2004 - PCA in studying coordination and variability a tu.pdf:application/pdf},
}

@article{dinc_evaluation_2014,
	title = {Evaluation of {Normalization} and {PCA} on the {Performance} of {Classifiers} for {Protein} {Crystallization} {Images}},
	volume = {2014},
	issn = {1091-0050},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4409005/},
	abstract = {In this paper, we investigate the performance of classification of protein crystallization images captured during protein crystal growth process. We group protein crystallization images into 3 categories: noncrystals, likely leads (conditions that may yield formation of crystals) and crystals. In this research, we only consider the subcategories of noncrystal and likely leads protein crystallization images separately. We use 5 different classifiers to solve this problem and we applied some data preprocessing methods such as principal component analysis (PCA), min-max (MM) normalization and z-score (ZS) normalization methods to our datasets in order to evaluate their effects on classifiers for the noncrystal and likely leads datasets. We performed our experiments on 1606 noncrystal and 245 likely leads images independently. We had satisfactory results for both datasets. We reached 96.8\% accuracy for noncrystal dataset and 94.8\% accuracy for likely leads dataset. Our target is to investigate the best classifiers with optimal preprocessing techniques on both noncrystal and likely leads datasets.},
	urldate = {2024-03-01},
	journal = {Proceedings of IEEE Southeastcon / IEEE Southeastcon. IEEE Southeastcon},
	author = {Din√ß, Imren and Sigdel, Madhav and Din√ß, Semih and Sigdel, Madhu S. and Pusey, Marc L. and Ayg√ºn, Ramazan S.},
	month = mar,
	year = {2014},
	pmid = {25914519},
	pmcid = {PMC4409005},
	keywords = {principal-component-analysis, preprocessing},
	pages = {10.1109/SECON.2014.6950744},
	file = {PubMed Central Full Text PDF:/home/ivan/Zotero/storage/WY2E9Q6X/Din√ß et al. - 2014 - Evaluation of Normalization and PCA on the Perform.pdf:application/pdf},
}

@article{liu_strategy_2014,
	title = {A {Strategy} on {Selecting} {Performance} {Metrics} for {Classifier} {Evaluation}:},
	volume = {6},
	issn = {1937-9412, 1937-9404},
	shorttitle = {A {Strategy} on {Selecting} {Performance} {Metrics} for {Classifier} {Evaluation}},
	url = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJMCMC.2014100102},
	doi = {10.4018/IJMCMC.2014100102},
	abstract = {The evaluation of classifiers‚Äô performances plays a critical role in construction and selection of classification model. Although many performance metrics have been proposed in machine learning community, no general guidelines are available among practitioners regarding which metric to be selected for evaluating a classifier‚Äôs performance. In this paper, we attempt to provide practitioners with a strategy on selecting performance metrics for classifier evaluation. Firstly, the authors investigate seven widely used performance metrics, namely classification accuracy, F-measure, kappa statistic, root mean square error, mean absolute error, the area under the receiver operating curve, and the area under the precision-recall curve. Secondly, the authors resort to using Pearson linear correlation and Spearman rank correlation to analyses the potential relationship among these seven metrics. Experimental results show that these commonly used metrics can be divided into three groups, and all metrics within a given group are highly correlated but less correlated with metrics from different groups.},
	language = {en},
	number = {4},
	urldate = {2024-03-01},
	journal = {International Journal of Mobile Computing and Multimedia Communications},
	author = {Liu, Yangguang and Zhou, Yangming and Wen, Shiting and Tang, Chaogang},
	month = oct,
	year = {2014},
	keywords = {metrics},
	pages = {20--35},
	file = {Liu et al. - 2014 - A Strategy on Selecting Performance Metrics for Cl.pdf:/home/ivan/Zotero/storage/ZJ4NGSBP/Liu et al. - 2014 - A Strategy on Selecting Performance Metrics for Cl.pdf:application/pdf},
}

@article{m_review_2015,
	title = {A {Review} on {Evaluation} {Metrics} for {Data} {Classification} {Evaluations}},
	volume = {5},
	issn = {2231007X, 22309608},
	url = {http://www.aircconline.com/ijdkp/V5N2/5215ijdkp01.pdf},
	doi = {10.5121/ijdkp.2015.5201},
	abstract = {Evaluation metric plays a critical role in achieving the optimal classifier during the classification training. Thus, a selection of suitable evaluation metric is an important key for discriminating and obtaining the optimal classifier. This paper systematically reviewed the related evaluation metrics that are specifically designed as a discriminator for optimizing generative classifier. Generally, many generative classifiers employ accuracy as a measure to discriminate the optimal solution during the classification training. However, the accuracy has several weaknesses which are less distinctiveness, less discriminability, less informativeness and bias to majority class data. This paper also briefly discusses other metrics that are specifically designed for discriminating the optimal solution. The shortcomings of these alternative metrics are also discussed. Finally, this paper suggests five important aspects that must be taken into consideration in constructing a new discriminator metric.},
	language = {en},
	number = {2},
	urldate = {2024-03-01},
	journal = {International Journal of Data Mining \& Knowledge Management Process},
	author = {M, Hossin and M.N, Sulaiman},
	month = mar,
	year = {2015},
	keywords = {metrics},
	pages = {01--11},
	file = {M and M.N - 2015 - A Review on Evaluation Metrics for Data Classifica.pdf:/home/ivan/Zotero/storage/2HZPNGUI/M and M.N - 2015 - A Review on Evaluation Metrics for Data Classifica.pdf:application/pdf},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efÔ¨Åcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conÔ¨Ågure neural networks and deep belief networks. Compared with neural networks conÔ¨Ågured by a pure grid search, we Ô¨Ånd that random search over the same domain is able to Ô¨Ånd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search Ô¨Ånds better models by effectively searching a larger, less promising conÔ¨Åguration space. Compared with deep belief networks conÔ¨Ågured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conÔ¨Åguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conÔ¨Åguring algorithms for new data sets. Our analysis casts some light on why recent ‚ÄúHigh Throughput‚Äù methods achieve surprising success‚Äîthey appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	keywords = {random-search-cv},
	file = {Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf:/home/ivan/Zotero/storage/XCNER8AN/Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf:application/pdf},
}

@article{borys_explainable_2023,
	title = {Explainable {AI} in medical imaging: {An} overview for clinical practitioners ‚Äì {Beyond} saliency-based {XAI} approaches},
	volume = {162},
	issn = {0720-048X},
	shorttitle = {Explainable {AI} in medical imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S0720048X23001006},
	doi = {10.1016/j.ejrad.2023.110786},
	abstract = {Driven by recent advances in Artificial Intelligence (AI) and Computer Vision (CV), the implementation of AI systems in the medical domain increased correspondingly. This is especially true for the domain of medical imaging, in which the incorporation of AI aids several imaging-based tasks such as classification, segmentation, and registration. Moreover, AI reshapes medical research and contributes to the development of personalized clinical care. Consequently, alongside its extended implementation arises the need for an extensive understanding of AI systems and their inner workings, potentials, and limitations which the field of eXplainable AI (XAI) aims at. Because medical imaging is mainly associated with visual tasks, most explainability approaches incorporate saliency-based XAI methods. In contrast to that, in this article we would like to investigate the full potential of XAI methods in the field of medical imaging by specifically focusing on XAI techniques not relying on saliency, and providing diversified examples. We dedicate our investigation to a broad audience, but particularly healthcare professionals. Moreover, this work aims at establishing a common ground for cross-disciplinary understanding and exchange across disciplines between Deep Learning (DL) builders and healthcare professionals, which is why we aimed for a non-technical overview. Presented XAI methods are divided by a method‚Äôs output representation into the following categories: Case-based explanations, textual explanations, and auxiliary explanations.},
	urldate = {2024-03-01},
	journal = {European Journal of Radiology},
	author = {Borys, Katarzyna and Schmitt, Yasmin Alyssa and Nauta, Meike and Seifert, Christin and Kr√§mer, Nicole and Friedrich, Christoph M. and Nensa, Felix},
	month = may,
	year = {2023},
	keywords = {explainable-ai},
	pages = {110786},
	file = {Full Text:/home/ivan/Zotero/storage/GCVEANYA/Borys et al. - 2023 - Explainable AI in medical imaging An overview for.pdf:application/pdf;ScienceDirect Snapshot:/home/ivan/Zotero/storage/RTGGGV7D/S0720048X23001006.html:text/html},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@misc{noauthor_shap_nodate,
	title = {{SHAP} explained the way {I} wish someone explained it to me {\textbar} by {Samuele} {Mazzanti} {\textbar} in {Towards} {Data} {Science} - {Freedium}},
	url = {https://freedium.cfd/https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30},
	urldate = {2024-03-01},
	file = {SHAP explained the way I wish someone explained it to me | by Samuele Mazzanti | in Towards Data Science - Freedium:/home/ivan/Zotero/storage/CWS8YIWC/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30.html:text/html},
}

@misc{noauthor_sklearnlinear_modelsgdclassifier_nodate,
	title = {sklearn.linear\_model.{SGDClassifier}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.linear_model.SGDClassifier.html},
	abstract = {Examples using sklearn.linear\_model.SGDClassifier: Model Complexity Influence Out-of-core classification of text documents Comparing various online solvers Early stopping of Stochastic Gradient Des...},
	language = {en},
	urldate = {2024-03-09},
	journal = {scikit-learn},
	file = {Snapshot:/home/ivan/Zotero/storage/9WJYGA8A/sklearn.linear_model.SGDClassifier.html:text/html},
}

@article{natekin_gradient_2013,
	title = {Gradient boosting machines, a tutorial},
	volume = {7},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021},
	doi = {10.3389/fnbot.2013.00021},
	abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods. A theoretical information is complemented with many descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. A set of practical examples of gradient boosting applications are presented and comprehensively analyzed.},
	language = {English},
	urldate = {2024-03-09},
	journal = {Frontiers in Neurorobotics},
	author = {Natekin, Alexey and Knoll, Alois},
	month = dec,
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {boosting, Classification, gradient boosting, machine learning, regression, robotic control, text classification},
	file = {Full Text PDF:/home/ivan/Zotero/storage/NNELQBFX/Natekin and Knoll - 2013 - Gradient boosting machines, a tutorial.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148‚Äì156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2024-03-09},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {regression, classification, ensemble},
	pages = {5--32},
	file = {Full Text PDF:/home/ivan/Zotero/storage/FCEBGR7X/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@misc{scikit_learn_bagging_nodate,
	title = {Bagging {Classifier}},
	url = {https://scikit-learn/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html},
	language = {en},
	urldate = {2024-03-09},
	journal = {scikit-learn},
	author = {{Scikit Learn}},
	file = {Snapshot:/home/ivan/Zotero/storage/S978JKGV/sklearn.ensemble.BaggingClassifier.html:text/html},
}

@article{wolpert_stacked_nodate,
	title = {{STACKED} {GENERALIZATION}},
	abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation‚Äôs crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
	language = {en},
	author = {Wolpert, David H},
	file = {Wolpert - STACKED GENERALIZATION.pdf:/home/ivan/Zotero/storage/JGXEJH77/Wolpert - STACKED GENERALIZATION.pdf:application/pdf},
}

@misc{hemashreekilari_understanding_2023,
	title = {Understanding {Bagging}},
	url = {https://medium.com/@hemashreekilari9/understanding-bagging-4b609ff7e4b7},
	abstract = {This is part two of the following sequence:},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Hemashreekilari},
	month = aug,
	year = {2023},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/F92Y6Q66/understanding-bagging-4b609ff7e4b7.html:text/html},
}

@misc{hemashreekilari_understanding_2023-1,
	title = {Understanding {Boosting}},
	url = {https://medium.com/@hemashreekilari9/understanding-boosting-2147e9393e59},
	abstract = {This is part three of the following sequence:},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Hemashreekilari},
	month = sep,
	year = {2023},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/89JRT97T/understanding-boosting-2147e9393e59.html:text/html},
}

@misc{hemashreekilari_understanding_2023-2,
	title = {Understanding {Random} {Forest}},
	url = {https://medium.com/@hemashreekilari9/understanding-random-forest-a87d08416280},
	abstract = {This is part three of the following sequence:},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Hemashreekilari},
	month = aug,
	year = {2023},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/HYC7N8WV/understanding-random-forest-a87d08416280.html:text/html},
}

@misc{hemashreekilari_understanding_2023-3,
	title = {Understanding {Gradient} {Boosting}},
	url = {https://medium.com/@hemashreekilari9/understanding-gradient-boosting-632939b98764},
	abstract = {This is part three of the following sequence:},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Hemashreekilari},
	month = sep,
	year = {2023},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/LURNLA6S/understanding-gradient-boosting-632939b98764.html:text/html},
}

@misc{ceballos_stacking_2019,
	title = {Stacking {Classifiers} for {Higher} {Predictive} {Performance}},
	url = {https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840},
	abstract = {Using the Wisdom of the Multiple Classifiers to Boost Performance},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Ceballos, Frank},
	month = sep,
	year = {2019},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/PXBPZ8YB/stacking-classifiers-for-higher-predictive-performance-566f963e4840.html:text/html},
}

@misc{shmueli_matthews_2020,
	title = {Matthews {Correlation} {Coefficient} is {The} {Best} {Classification} {Metric} {You}‚Äôve {Never} {Heard} {Of}},
	url = {https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a},
	abstract = {Still using accuracy and F1-score? Time to upgrade.},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Shmueli, Boaz},
	month = may,
	year = {2020},
	file = {Snapshot:/home/ivan/Zotero/storage/56W434KW/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a.html:text/html},
}

@misc{noauthor_matthewss_nodate,
	title = {Matthews‚Äôs correlation coefficient: {Definition}, {Formula} and advantages - {Voxco}},
	shorttitle = {Matthews‚Äôs correlation coefficient},
	url = {https://www.voxco.com/blog/matthewss-correlation-coefficient-definition-formula-and-advantages/},
	abstract = {Matthew‚Äôs correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table.},
	language = {en-US},
	urldate = {2024-03-12},
	note = {Section: Blog},
	file = {Snapshot:/home/ivan/Zotero/storage/6Z7ER9XB/matthewss-correlation-coefficient-definition-formula-and-advantages.html:text/html},
}

@misc{sisters_matthews_2020,
	title = {Matthews {Correlation} {Coefficient}: when to use it and when to avoid it},
	shorttitle = {Matthews {Correlation} {Coefficient}},
	url = {https://towardsdatascience.com/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e},
	abstract = {It‚Äôs not a silver bullet metric to classification problems},
	language = {en},
	urldate = {2024-03-12},
	journal = {Medium},
	author = {Sisters, Li},
	month = may,
	year = {2020},
	file = {Snapshot:/home/ivan/Zotero/storage/45ZR3GWT/matthews-correlation-coefficient-when-to-use-it-and-when-to-avoid-it-310b3c923f7e.html:text/html},
}

@misc{brownlee_how_2018,
	title = {How to {Use} {ROC} {Curves} and {Precision}-{Recall} {Curves} for {Classification} in {Python}},
	url = {https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/},
	abstract = {It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly. This flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, [‚Ä¶]},
	language = {en-US},
	urldate = {2024-03-12},
	journal = {MachineLearningMastery.com},
	author = {Brownlee, Jason},
	month = aug,
	year = {2018},
	keywords = {images},
	file = {Snapshot:/home/ivan/Zotero/storage/HZLGJF4I/roc-curves-and-precision-recall-curves-for-classification-in-python.html:text/html},
}

@misc{noauthor_cross_2023,
	title = {Cross {Validation}, {Explained} - {Sharp} {Sight}},
	url = {https://www.sharpsightlabs.com/blog/cross-validation-explained/},
	abstract = {This blog post explains cross validation in machine learning. It explains what cross validation is, different types, and specific challenges with CV.},
	language = {en-US},
	urldate = {2024-03-13},
	month = dec,
	year = {2023},
	note = {Section: machine learning},
	keywords = {image},
	file = {Snapshot:/home/ivan/Zotero/storage/J2ZWIEN9/cross-validation-explained.html:text/html},
}
