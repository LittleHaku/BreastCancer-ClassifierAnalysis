To build the classifiers, \textit{Scikit-learn} was used. This library was chosen because it is the most popular library for building machine learning models in \textit{Python} and it has a wide variety of classifiers and ensemble methods. The classifiers were built in a \textit{Jupyter Notebook} to allow reproducibility and to facilitate the process of building and optimizing the classifiers. The classifiers can be divided into two types, base and ensemble, base classifiers are the simplest classifiers, and ensemble classifiers are a combination of base classifiers.

The base classifiers used are:
\begin{itemize}
    \item \textbf{Logistic Regression}
    \item \textbf{Multilayer Perceptron}
    \item \textbf{Support Vector Machine}
    \item \textbf{Decision Tree}
    \item \textbf{Stochastic Gradient Descent}
    \item \textbf{K-Nearest Neighbors}
    \item \textbf{Linear Discriminant Analysis}
    \item \textbf{Gaussian Naive Bayes}
    \item \textbf{Quadratic Discriminant Analysis}
\end{itemize}

Based on the best results of the base classifiers, the ensemble classifiers were built. Some of these ensemble classifiers allow one to choose the base classifier to use, for these cases, the best base classifiers were used. The ensemble classifiers used are:
\begin{itemize}
    \item \textbf{Random Forest}
    \item \textbf{AdaBoost}
    \item \textbf{Gradient Boosting}
    \item \textbf{Bagging}
    \item \textbf{Voting}
    \item \textbf{Stacking}
\end{itemize}

\subsection{Comparison of Classifiers}

To decide if a classifier is better than another, evaluation metrics were used, and to choose the appropriate metrics for this problem, we first need to understand the basic metrics used in classification problems \cite{m_review_2015}:

\begin{itemize}
    \item \textbf{Accuracy}: Proportion of correct predictions over the total number of instances evaluated. $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$
    \item \textbf{Recall}: Proportion of true positive results among the number of all positive instances. $Recall = \frac{TP}{TP + FN}$
    \item \textbf{Precision}: The proportion of true positive results among the number of cases that were predicted to be positive. $Precision = \frac{TP}{TP + FP}$
    \item \textbf{Specificity}: The proportion of true negative results among the number of cases that were actually negative. $Specificity = \frac{TN}{TN + FP}$
\end{itemize}

Now we have to choose the appropriate metrics for this problem, the most important thing for this problem is to minimize the number of false negatives, since a false negative means that a patient with cancer was classified as healthy, and this could have fatal consequences. To minimize the number of false negatives, we will use the \textit{Recall}, but the problem of only taking into account the recall is that to maximize it, the model could classify all samples as positive, to avoid this we will use the \textit{Precision} since this will penalize the model for classifying a sample as positive if it was not. To combine these two metrics we will use the \textit{F1 Score}, which is the harmonic mean of the precision and recall. We will also use the \textit{F2 Score}, which is the weighted harmonic mean of the precision and recall, this will give more importance to the recall since we want to minimize the number of false negatives \cite{rutecki_best_nodate}. So, the evaluation metrics sorted by the importance that we will use are:

\begin{itemize}
    \item \textbf{F1 Score}: $$F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$$
    \item \textbf{Recall}: $$Recall = \frac{TP}{TP + FN}$$
    \item \textbf{F2 Score}: $$F2 = 5 \times \frac{Precision \times Recall}{\frac{4}{Precision} + \frac{1}{Recall}}$$
    \item \textbf{Precision}: $$Precision = \frac{TP}{TP + FP}$$
\end{itemize}

\subsection{Optimization of Classifiers}

To maximize these metrics in the classifiers, we have to optimize the hyperparameters of the classifiers. The hyperparameters are the parameters that are not learned by the model, and they are set before the training process, these can, for example, be the number of neighbors in the K-Nearest Neighbors classifier, the number of neurons in the Multilayer Perceptron classifier, or the maximum depth of the Decision Tree classifier. To optimize these hyperparameters, we used the \textit{RandomizedSearchCV} followed by the \textit{GridSearchCV}. The reason for using the \textit{RandomizedSearchCV} first is that can achieve similar results to the \textit{GridSearchCV} but with a lower computational cost \cite{bergstra_random_nodate}, this will allow us to explore a wider range of hyperparameters and have a range of values to use in the \textit{GridSearchCV}. Then we used the \textit{GridSearchCV} to find the best hyperparameters in the range of values found by the \textit{RandomizedSearchCV}.

\newacronym{cv}{CV}{Cross-Validation}

These searches had a 5-fold \ac{cv}, which means that the dataset was split into 5 parts, 4 parts were used for training and 1 part was used for testing, this process was repeated 5 times, each time with a different part for testing, and the results were averaged, using the \textit{F1 Score} as the metric to optimize. All the \textit{RandomizedSearchCV} and \textit{GridSearchCV} were done using the same seed, this means that the split of the dataset was the same for all the classifiers, this ends up in a more reliable result and also allows the reproducibility of the results. This process was done to avoid overfitting and to have a more reliable result, of course, this process was only done in the training set, to avoid data leakage.