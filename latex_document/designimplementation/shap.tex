Building and optimizing the classifiers is a crucial step and can be helpful in a clinical situation, but it can only get us so far. We also need to know why the model made a certain decision, for this, there is a branch of \ac{ai} called \acl{xai}, this branch of \ac{ai} is focused on making \ac{ml} models more than a black box which is fed with an input and provides an output, it is focused on making these models transparent, allowing clinical practitioners to understand why a certain decision was made \cite{borys_explainable_2023}. To achieve this we will use the \acl{shap} library. The reason for using \ac{shap} is that it has been proven to achieve great explanations for \ac{ml} models in the medical field \cite{massafra_analyzing_2023}.

\ac{shap} \cite{lundberg_unified_2017} is a model-agnostic approach to \ac{xai}, that is, it can be used with any model since it only uses the input and output. In short, how \ac{shap} works, is that it uses the Shapley values from cooperative game theory to explain the output of any machine learning model. The Shapley value is a concept from cooperative game theory, it is a way to fairly distribute the loot obtained by a coalition of players. The difference is that in \ac{shap} instead of players we have features and instead of loot, we have the predicted output as a probability. The Shapley value is the average contribution of a feature to the prediction, and it is calculated by averaging the marginal contributions of a feature to the prediction over all possible orderings of the features. The marginal contribution of a feature to the prediction is the difference between the prediction with the feature and the prediction without the feature. This is done for all possible subsets of features, and then the Shapley value is the average of all these marginal contributions. This is done for all the features, and then we have the Shapley values, which are the importance of the features for the prediction.

With this in mind, we will use \ac{shap} to have a global understanding of the importance of the features in the models and a local understanding of how a certain decision was made. This will allow us to understand which features are the most important for a classifier, how sure the classifier was of its prediction, and which features determined if the sample was benign or malignant. This will also be done in the \textit{Jupiter Notebook} to ensure reproducibility. To be able to calculate the \ac{shap} values, we need to create an \textit{Explainer} object, this object is created with the classifier as a parameter, but in our case, we are not using just the classifier but pipelines, this means, that we will have to find a way to obtain the predicted probability of the pipeline. To do this, we will use the \textit{predict\_proba} method of the pipeline, this method will return the predicted probability of the sample being malignant no matter which classifier is used, with this, we will be able to create the \textit{Explainer} object and calculate the \ac{shap} values, all this process is show in the Code Snippet \ref{PY:SHAP}.


\PythonCode[PY:SHAP]{Obtaining the predicted probability of the pipeline and calculating the SHAP values}{Obtaining the predicted probability of the pipeline and calculating the SHAP values}{explainer.py}{1}{12}{1}