Once we have done the \ac{eda} and we know how the dataset is structured, how the different features are distributed, and the relationships between the features, we can start preprocessing the dataset. To achieve this we will standardize the features and then reduce the dimensionality of the features by using \ac{pca}, this will result in a dataset that is easier to work with and that will allow us to build better classifiers \cite{dinc_evaluation_2014}. To facilitate this process we will use pipelines, which will allow us to easily build and optimize the classifiers and to ensure reproducibility and prevent data leakage \cite{zhao_pre-process_2019}, this means that we 
won't 
directly have to modify the dataset. The code snippet \ref{PY:PIPELINE} shows how the pipeline is built.

\PythonCode[PY:PIPELINE]{Building the pipeline}{Building the pipeline with the StandardScaler and PCA.}{pipeline.py}{1}{5}{1}

